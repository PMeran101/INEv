\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{titling} % For redefining the date
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,     % Enable colored links
    linkcolor=blue,      % Color for internal links
    urlcolor=blue        % Color for external links (URLs)
}
\title{Reproducibility and Availability:\\ INEv: In-Network Evaluation for Event Stream Processing}


\author{Samira Akili \and 
	Steven Purtzel \and
	Matthias Weidlich \\
	Humboldt-UniversitÃ¤t zu Berlin
	}

\date{} % Redefine the \date command to be empty

\begin{document}

\maketitle
In-Network (INEv) evaluation graphs provide a model for the in-network evaluation of Complex Event Processing (CEP) queries. Given a query workload, network topology, and statistics about the event generation in the network, as well as query selectivities, an INEv graph compactly describes (i) how the queries of the workload should be split into sub-queries (query projections), (ii) where the subqueries should be placed in the network, and (iii) how events, matches of projections, and partial results thereof shall be forwarded in the network to reduce network communication.

This document provides a comprehensive guide on reproducing the experimental results of our paper and describes the utilization of INEv for the in-network evaluation of arbitrary workloads. Should you require any assistance with the evaluation or have any inquiries, please do not hesitate to contact us at \url{akilsami@hu-berlin.de}.

\section{Availability}


Our code is available as open-source projection on Git-Hub: \url{https://github.com/samieze/INEv}.
The repository contains the code for the INEv graph construction and for the generation of input networks and query workloads. 
While the main contribution of our work is the efficient construction of INEv graphs, we also provide a prototype distributed CEP (DCEP) engine, that enables INEv graph based query evaluation on both synthetic data and real-world datasets. 
The repository also includes an archive that contains the scripts and data needed to reproduce the experiments from our paper.


\section{Reproducibility}
We provide full reproducibility of all experiments described in our paper. 
\\
\\
\textbf{Download Materials.} To replicate our experiments, you must first clone the GitHub repository (\url{https://github.com/samieze/INEv}). Then navigate to the folder \textit{Reproducibility\_Submission} and unpack both contained zipped archives (\textit{reproducibility.tar.xz} and \textit{data.tar.xz}). Merge the \textit{reproducibility} folder inside the \textit{data} folder and the \textit{reproducibility} folder you just unpacked by executing the command \texttt{'rsync -av --ignore-existing data/reproducibility/ \\ reproducibility'} inside the \textit{Reproducibility\_Submission} folder. The resulting \textit{reproducibility} folder contains the complete code and data necessary to conduct all experiments.
\\
\\
Our experiments are categorized in two groups: The first group contains experiments that can be run on a single machine. For this group, we provide a single script that starts all experiments and generates plots for the experiment  results. The (local) experiments can be found in the folder \textit{reproducibility/local\_experiments}. 

The second group of experiments have to be executed in a distributed environment. We provide an extensive instruction on how to set up a Raspberry Pi cluster in order to run the experiments as well as scripts that automate the remote execution of experiments, the collection of experiment results and the generation of plots. The (remote) experiments can be found in the folder \textit{reproducibility/pi\_cluster\_experiments}.


\subsection{Local Experiments}

The main part of our experimental results can be reproduced by envoking the script \textit{./start\_all.sh} within the folder \textit{reproducibility/local\_experiments}. The total runtime of the script should be around \textbf{72 hours}. After the scripts termination the folder \textit{reproducibility/local\_experiments/Figures} should contain the figures \textbf{Fig.4, Fig.6-Fig.10} as well as \textbf{Table 3}.
\\
\\
\textbf{Hardware Configuration.}
The system requirements for running the script are:
\begin{enumerate}
\item An x86-64 machine running Ubuntu 18.04 or later.
\item Python version 3.8.
\item openjdk 11.0.19.
\item At least 8 logical cores.
\end{enumerate}
We tested the script on a server running Intel Xeon E7-4880 at 2.50GHz with 1TB RAM and 60 logical cores.
\\
\\
\textbf{Dependencies.}
The list of used packages is given in \textit{all\_requirements.txt} (inside the folder \textit{reproducibility/local\_experiments}).
Please install the required packages, i.e., by running \textit{'pip3 install -r all\_requirements.txt'} from inside the folder \textit{reproducibility/local\_experiments}. 
\\
\\
\subsubsection{Local Experiment Subfolders.}
In this section, we give a brief overview over the different experiments in the \textit{reproducibility/local\_experiments} folder, their expected results and runtime. However, as stated above, all of the local experiments can be executed by envoking the script \textit{./start\_all.sh}.
\\
\\
\textbf{REPRO\_basic} contains the version of our algorithms as they can be found in the github repository. This version is used to generate INEv graphs under varying network and workload characteristics. Moreover the folder contains the experiments that compare the costs of INEv graphs to a lower bound and investigate the computation time. The following plots are generated by envoking \textit{./REPRO\_basic/scripts/combined.sh}:
\begin{itemize}
\item Fig. 4e
\item Fig. 6a-d
\item Fig. 7a
\item Fig. 10
\end{itemize}
The experiment takes about \textbf{72 hours} to finish.
\\
\\
\textbf{REPRO\_adaptivity} contains experiments on the robustness of INEv graphs as well as the benefit of the adaptive repair proposed in Section 6.6 of our paper.
The following plots are generated by envoking
\textit{./REPRO\_adaptivity/scripts/\\combined.sh}.
\begin{itemize}
\item Fig. 8a-d
\item Fig. 9c-d
\end{itemize}
The experiment takes about \textbf{24 hours} to finish.
\\
\\
\textbf{REPRO\_multiquery} contains an experiment that investigates the benefit of our approach for multi-query sharing as described in Section 6.3.
The following plot is generated by envoking \textit{./REPRO\_multiquery/scripts/combined.sh}:
\begin{itemize}
\item Fig. 7d
\end{itemize}
The experiment takes about \textbf{10 hours} to finish.
\\
\\
\textbf{REPRO\_muse\_INev} contains the experiments conducted to compare the transmission costs of MuSE and INEv graphs for networks with varying characteristics (event rate distribution, network size) as part of the State-of-the-art comparison. The following plots are generated by envoking  \textit{./REPRO\_muse\_Inev\\/scripts/combined.sh}:
\begin{itemize}
\item Fig. 4a-b
\end{itemize}
The experiment takes about \textbf{12 hours} to finish.
\\
\\
\textbf{REPRO\_timewindows} contains experiments on the influence of different timewindows for queries of the same workload under varying workload sizes.
The following plots are generated by envoking \textit{./REPRO\_timewindows/scripts/\\combined.sh}:
\begin{itemize}
\item Fig. 9a-b
\end{itemize}
The experiment takes about \textbf{12 hours} to finish.
\\
\\
\textbf{REPRO\_wlorder} contains an experiment that investigates the influence of the order in which the queries of a workload are processed by our algorithm. The following plots are generated by envoking \textit{./REPRO\_wlOrder/scripts/\\combined.sh}:
\begin{itemize}
\item Fig. 7c
\end{itemize}
The experiment takes about \textbf{8 hours} to finish.
\\
\\
\textbf{REPRO\_PPoP\_INev} contains the experiments conducted to compare the transmission costs of the distributed evaluation plans proposed by the PPoP approach with INEv graphs for networks with varying event rate distribution.
The following plots are generated by envoking \textit{./REPRO\_PPoP\_INev/scipts\\/combined.sh}:
\begin{itemize}
\item Fig. 4a-b
\end{itemize}
The experiment takes about \textbf{8 hours} to finish.
\\
\\
\textbf{REPRO\_variance} contains the experiment that investigate the influence of variance in the event rates on the network costs of a given INEv graph. The experiment evaluates 10 INEv graphs with our distributed CEP engine. The 10 INEv graphs used are the same as used for the evaluation of the paper. The following plot is generated by envoking \textit{./REPRO\_variance/scripts/combined.sh}:
\begin{itemize}
\item Fig. 8e
\end{itemize}
The experiment takes about \textbf{12 hours} to finish.
\\
\\
\textbf{REPRO\_table.} contains all experiments that need to be run in order to obtain Table 3. The experiment evaluates different query workloads on different partitionings of the two real-world data sets google(cluster) and citibike based on distributed evaluation plans obtained by INEv, MuSE, PPoP and a simple hash partitioning scheme inspired by Flink. 
By running the script \textit{REPRO\_table/scipts/combined.sh} a table comprising the results of \textbf{Table 3} is generated. 
The experiment runs for about \textbf{24 hours}. 

A row in the resulting table yields the transmission ratio for an evaluation plan generated by an approach for a given query and network whereas the network is obtained by partitioning the data set with a given partitioning size. The table schema is given by \textsc{[Dataset, Query, PartitioningSize, TransmissionRatio, Terminated, Approach]}. The field \textsc{Terminated} indicates if the evaluation terminated. If value of \textsc{Terminated} is set to "False", this corresponds to the cells in Table 3 of the paper containing a ">".
%\\
%Troubleshooting: For the experiments contained in \textbf{REPRO\_table} multiple runs of our DCEP engine are started in parallel to evaluate INEv and muse graphs. If a run fails, it shows by a "0" in the table for the corresponding \textsc{Transmission Ratio} value.
%
%A failed experiment can be restarted by running from inside the \textit{REPRO\_table} directory: 
%
%\texttt{timeout 45m ./bin/DCEP.Simulation \\google/\$query/\$partitioning/\$approach\_google\_Q"\$query"\_"\$partitioning".txt -w 1 -d 43 -t Minute --name ex >\\ \$approach\_google\_Q"\$query"\_"\$partitioning"\_out.txt}
%if it is from the google dataset and by running
%\textit{'timeout 45m ./bin/DCEP.Simulation citibike/$partitioning/$approach/$approach_citibike_Q"$query"\_"$partitioning".txt -w 1 -d 43 -t Minute --name ex> $approach\_citibike\_Q"$query"\_"$partitioning"\_out.txt} if it is from the citibike dataset by substituting the respective values for \$approach, \$query and \$partitioning respectively according to the row in the table.
%After running new experiments, the python program \textit{'get\_res\_inev\_muse.py'} (in the \textit{REPRO\_table} directory) must be started again to process the result files and update the table. 
\\
\\
\textbf{Interpretation of Results.} All the experiments mentioned above utilize at least one of the following techniques: randomized event rate generation, randomized selectivity generation, randomized query workload generation, or randomized network topology generation. Consequently, the resulting plots may not be identical to those displayed in the paper. However, the trends and relationships observed in the presented data should still support all the key assertions made in the paper.

\subsection{Pi Cluster Experiments}

The folder \textit{reproducibility/\_pi\_cluster\_experiments} contains the subset of our experiments that need to be executed on a real-world distributed environment, a Raspberry Pi Cluster. The folder contains the additional \textit{readme.md} file which explains how to set up and prepare the Pi cluster, run the experiments and generate the respective plots. Following the instructions of the contained \textit{readme.md} file to execute the experiments, the plots generated are: \textbf{Fig. 5a-d}. The experiments takes about \textbf{72 hours} to finish.




\end{document}
